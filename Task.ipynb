{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, yaml\n",
    "from Model import GPT2S\n",
    "import torch.nn.functional as F\n",
    "from encoder import get_encoder\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4510/1252070071.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  config = AttrDict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = AttrDict(yaml.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2S(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=5e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=5e-05, elementwise_affine=True)\n",
       "        (mlp): FeedForward(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=5e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): LMHead(\n",
       "    (decoder): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2S(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight(model, state_dict):\n",
    "    old_keys = []\n",
    "    new_keys = []\n",
    "    for key in state_dict.keys():\n",
    "        new_key = None\n",
    "        if key.endswith(\".g\"):\n",
    "            new_key = key[:-2] + \".weight\"\n",
    "        elif key.endswith(\".b\"):\n",
    "            new_key = key[:-2] + \".bias\"\n",
    "        elif key.endswith(\".w\"):\n",
    "            new_key = key[:-2] + \".weight\"\n",
    "        if new_key:\n",
    "            old_keys.append(key)\n",
    "            new_keys.append(new_key)\n",
    "    for old_key, new_key in zip(old_keys, new_keys):\n",
    "        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "    missing_keys = []\n",
    "    unexpected_keys = []\n",
    "    error_msgs = []\n",
    "    # copy state_dict so _load_from_state_dict can modify it\n",
    "    metadata = getattr(state_dict, \"_metadata\", None)\n",
    "    state_dict = state_dict.copy()\n",
    "    if metadata is not None:\n",
    "        state_dict._metadata = metadata\n",
    "\n",
    "    def load(module, prefix=\"\"):\n",
    "        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
    "        module._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs\n",
    "        )\n",
    "        for name, child in module._modules.items():\n",
    "            if child is not None:\n",
    "                load(child, prefix + name + \".\")\n",
    "\n",
    "    start_model = model\n",
    "    if hasattr(model, \"transformer\") and all(not s.startswith('transformer.') for s in state_dict.keys()):\n",
    "        start_model = model.transformer\n",
    "    load(start_model, prefix=\"\")\n",
    "\n",
    "    # Make sure we are still sharing the output and input embeddings after loading weights\n",
    "    model.set_tied()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    values, _ = torch.topk(logits, k)\n",
    "    min_values = values[:, -1]\n",
    "    return torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
    "\n",
    "def sample_sequence(model, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, sample=True):\n",
    "    context = torch.full((batch_size, 1), start_token, dtype=torch.long)\n",
    "    prev = context\n",
    "    output = context\n",
    "    past = None\n",
    "    with torch.no_grad():\n",
    "        for i in trange(length):\n",
    "            logits, past = model(prev, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            log_probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                prev = torch.multinomial(log_probs, num_samples=1)\n",
    "            else:\n",
    "                _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "            output = torch.cat((output, prev), dim=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('pytorch_model.bin', map_location='cpu' if not torch.cuda.is_available() else None)\n",
    "tokenizer = get_encoder()\n",
    "model = load_weight(model, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Just tell me anything\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:11<00:00, 22.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catcher\n",
      "\n",
      "After pre-registration, any precompensated members who have photo identification will be presented with online questions and will be notified of their initial biographic confirmation. When nominations are completed, the party chair will arrange to solicit submissions once an administrative appeal is completed; if the appeal is pending, the chair will explain the reasons behind its conduct as: Insurance technology doesn't work; Government is concerned; A sponsor is sponsoring this party who is compromised or financially compromised.\n",
      "\n",
      "Registered non-members are responsible for making sure the Canadian Taxpayers Federation is in compliance to the Canadian IRS Electronic System so that the Commissioners cannot correct any classification duces d'elys.[1]\n",
      "\n",
      "A reply to the Commissioner's letter.\n",
      "\n",
      "Letter mark and gilded memberhips\n",
      "\n",
      "Available alumnae for re-election are fully repayable. Financial reserves exceeding $20,000 are granted.\n",
      "\n",
      "Telephone tickets.\n",
      "\n",
      "Men can attend any public (including MFBN) event or where tickets are sold on an RBS retail operator site – Interpretation Home in Saint John, London.\n",
      "\n",
      "Applications for any campaign materials guaranteeing that the applicant is an accountant or a financial planner must be received by October 2, 2017 and are\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context_tokens = tokenizer.encode(text)\n",
    "generated = 0\n",
    "for _ in range(1):\n",
    "    out = sample_sequence(\n",
    "            model=model, length=256,\n",
    "            context=context_tokens,\n",
    "            start_token=tokenizer.encoder['<|endoftext|>'],\n",
    "            batch_size=1\n",
    "        )\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    for i in range(1):\n",
    "        generated += 1\n",
    "        text = tokenizer.decode(out[i])\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
